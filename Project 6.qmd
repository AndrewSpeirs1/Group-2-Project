---
title: "Impact of Age, Gender, Socio-Economic Status, and Lifestyle on BMI Distrubution"
author: Yining Jin, James Robb, Kiran Sandhu, Andrew Speirs
format: pdf
editor: visual
number-sections: true
---

```{r}
#| echo: false
#| warning: false
#| message: false
library(readr)
library(ggplot2)
library(ggfortify)
library(tidyverse)
library(tidymodels)
library(gt)
library(MASS)
library(patchwork)
library(moderndive)
library(scales)
library(dplyr)
library(data.table)
```

# Introduction {#sec-intro}

# Exploratory Analysis {#sec-EA}

@tbl-summary shows the Mean, Median and Standard deviation of the BMI results from 2013 - 2016. Looking at @tbl-summary Below, we can see that the mean and median values for the BMI have a slight increase from 2013 to 2016, which would suggest that there is a population-wide trend of increasing BMI values.

Also, the standard deviation results provide information about the variability or spread of the BMI values within each of the years. The standard deviation results from @tbl-summary also increase from the 2013 - 2016, indicating that there is more diversity in the BMI results in 2015 and 2016.

```{r}
#| eval: true
#| echo: false
#| warning: false
#| message: false

Data <- read.csv("DAProject6.csv", fileEncoding = "UTF-8")
```

```{r}
#| echo: false
#| warning: false
#| label: tbl-summary
#| tbl-cap: Mean, Median and Standard deviation of the BMI from 2013-2016

Data.Year <- Data |>
  dplyr::select(BMI, Year)
Data.Year.Sum <- Data.Year |>
  group_by(Year) |>
  summarise(Mean = mean(BMI), Median = median(BMI), SD = sd(BMI)) |>
  gt() |>
  fmt_number(columns = -Year, decimals = 2) |>
   tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels()
  )

Data.Year.Sum
```

@fig-boxhist shows that despite the similar median and quartile values, there are notable differences in the spread and variability of the BMI results across the 4 years, with 2015 and 2016 having the most outliers. The presence of many outliers suggests that there is substantial variability and dispersion of BMI results for each year.

Also, Looking at @fig-boxhist, we can see that the histogram is slightly right skewed with the majority of the observations on the left-hand side of the histogram and a long tail extending towards the right. A right skewed histogram is also known as a positively skewed histogram and the peak of the graph can be found on the left-hand side and more specifically, between 20-30 BMI from the results.

```{r}
#| echo: false
#| warning: false
#| label: fig-boxhist
#| fig-cap: Boxplot for the BMI results for each year(Left) and Histogram of the BMI(Right)
#| fig-width: 9
#| fig-height: 3.5

par(mfrow = c(1,2))

year_colors <- c("red", "blue", "green", "purple")

plot(factor(Data$Year), Data$BMI, xlab = "Year", ylab = "BMI", col = year_colors)

hist(Data$BMI, main = "", xlab = "BMI", col = "skyblue")

```

# Formal Analysis {#sec-FA}

Analysis was conducted using R Studio and various packages. The significance level for all model tests was $\alpha = 0.05$.

```{r}
#| echo: false
#| warning: false
#| message: false
library(dplyr)
library(psych)
library(ggplot2)
library(ggpubr)

Data <- read.csv("DAProject6.csv")

```

## Question 1

We start by fitting a linear model about the relationship between BMI in Scotland and the given years. To assess $H_0:$ "Year is not a significant predictor for BMI", the following linear model, model1, was fitted:

$$ 
\begin{aligned}
y_i &= \beta_0 + \beta_1 \cdot x_{1i} + \beta_2 \cdot x_{2i} + \beta_3 \cdot x_{3i} + \epsilon_i, \quad \epsilon_i \sim N(0,\sigma^2), \quad i=1,...,14017, \\ 
&= \beta_0 + \beta_{2014} \cdot \mathbb{I}_{\mbox{2014}}(x) + \beta_{2015} \cdot \mathbb{I}_{\mbox{2015}}(x) + \beta_{2016} \cdot \mathbb{I}_{\mbox{2016}}(x) + \epsilon_i,
\end{aligned}
$$

Where

-   $\beta_0$ is the intercept of the regression line for the baseline year (2013);

-   $\beta_{year}$ is the additional term added to $\beta_0$ to get the intercept of the regression line for the specified year;

-   $\mathbb{I}_{\mbox{year}}(x)$ is an indicator function indicating the chosen year.

```{r}
#| eval: true
#| echo: false
#| warning: false
#| message: false
#| label: tbl-m1coef
#| tbl-cap: Estimates of the model1 coefficients.

Data$Year.2014 <- ifelse(Data$Year == 2014, 1, 0)
Data$Year.2015 <- ifelse(Data$Year == 2015, 1, 0)
Data$Year.2016 <- ifelse(Data$Year == 2016, 1, 0)

model1 <- lm(BMI ~ Year.2014 + Year.2015 + Year.2016, data = Data)
               
get_regression_table(model1)[, c(1,2,3,4,5)] |>
  gt() |>
  fmt_number(decimals=3) |>
  tab_style(style = cell_text(weight = "bold"),
            locations = cells_column_labels())


```

Results show that the 'Year' predictor is not statistically significant with BMI as all of the p-values are greater than 0.05. So, we are unable to reject the null hypothesis and we conclude that there might be no difference in average BMI across the given years. Now, let's look at the assumptions of this model.

@fig-Ass1 displays the residuals versus fitted values (Left) and histogram of the Residuals (Right). Although there appears to be a greater number of data points distributed above the zero line in the graph, given the size of the dataset, we can make the assumption that the mean of the data is centered around zero. Therefore, we can conclude that the assumptions regarding the residuals, specifically that they have mean zero and constant variance across all of the fitted values, are satisfied. Now, looking at the histogram, the residuals appear to be bell-shaped and they seem to be centered at zero. Even though there seems to be some right skewness in the histogram, due to the large dataset, we can still conclude that the assumptions of constant variance and mean zero for the residuals can be satisfied.

```{r}
#| echo: false
#| warning: false
#| label: fig-Ass1
#| fig-cap: Residuals V Fitted values(Left) and Histogram of Residuals(Right)
#| fig-align: center
#| fig-width: 9
#| fig-height: 3.5

par(mfrow = c(1,2))

plot(model1, which = 1, main = "")

hist(model1$residuals, main = "", xlab = "Residuals")
```

## Question 2

```{r}
#| echo: false
#| warning: false
#| message: false
library(tidyverse)
library(tidymodels)
library(moderndive)
library(gt)
library(patchwork)
library(ggplot2)
library(MASS)
bmi<- read.csv("DAProject6.csv")
```

For the rest of the variables, namely age, sex, highest education qualification attained, sufficient vegetable intake and sufficient fruit intake, we can fit linear regression models to each of these individually to see whether they have a significant impact on the BMI of an individual. The table below represents the results from all 5 of these models that we have fitted.

| Term                               | estimate | p-value |
|:-----------------------------------|:--------:|:-------:|
| intercept                          |  25.636  |  0.000  |
| Age                                |  0.045   |  0.000  |
| intercept                          |  27.898  |  0.000  |
| Sex: Male                          |  0.056   |  0.544  |
| intercept                          |  27.234  |  0.000  |
| Education: Higher grade or equiv   |  0.357   |  0.012  |
| Education: HNC/D or equiv          |  0.929   |  0.000  |
| Education: No qualifications       |  1.756   |  0.000  |
| Education: Other school level      |  1.440   |  0.000  |
| Education: Standard grade or equiv |  0.800   |  0.000  |
| intercept                          |  28.018  |  0.000  |
| Veg: Yes                           |  -0.122  |  0.267  |
| intercept                          |  27.752  |  0.000  |
| Fruit: Yes                         |  0.248   |  0.013  |

: estimates of the model coefficients

```{r}
#| echo: false 

bmi_a<-bmi|>
  dplyr::select(Age,Sex,Education, Veg, Fruit, BMI)

model2<- linear_reg()|>
  fit(BMI~Age, data=bmi)

model3<- linear_reg()|>
  fit(BMI~Sex, data=bmi)

model4<- linear_reg()|>
  fit(BMI~Education, data=bmi)

model5<- linear_reg()|>
  fit(BMI~Veg, data=bmi)

model6<- linear_reg()|>
  fit(BMI~Fruit, data=bmi)

t1<-as.data.frame(get_regression_table(model2$fit)[,c(1,2,5)]|>
  gt()|>
  fmt_number(decimals = 3))

t2<-as.data.frame(get_regression_table(model3$fit)[,c(1,2,5)]|>
  gt()|>
  fmt_number(decimals = 3))

t3<-as.data.frame(get_regression_table(model4$fit)[,c(1,2,5)]|>
  gt()|>
  fmt_number(decimals = 3))

t4<-as.data.frame(get_regression_table(model5$fit)[,c(1,2,5)]|>
  gt()|>
  fmt_number(decimals = 3))

t5<- as.data.frame(get_regression_table(model6$fit)[,c(1,2,5)]|>
  gt()|>
  fmt_number(decimals = 3))
```

Hence, from Table 3 we obtain the following regression lines:

$$
\begin{aligned}
\widehat{\mbox{BMI}} =~&25.64 + 0.05 \cdot Age \\
\widehat{\mbox{BMI}} =~&27.23 + 0.36 \cdot (Education: Higher~grade~or~equiv) + 0.93 \cdot (Education: HNC/D~or~equiv) + \\ &1.76 \cdot (Education: No~qualifications) + 
1.44 \cdot (Education: Other~School~level) + \\ &0.80 \cdot (Education: Standard~grade~or~equiv) \\
\widehat{\mbox{BMI}} =~&27.75 + 0.25 \cdot (Fruit: Yes)
\end{aligned}
$$

From all of these models that have been fitted in table 5 , we can observe the p-values of each of these, and we get the following results from them. The variables Age, Education and Fruit are all statistically significant with BMI distribution whilst Sex and Veg intake are not.

Now, since we have chosen three models that are statistically significant (Age, Education and Fruit), we can now test for the assumptions of each of these models.

We shall begin with the Age model

```{r}
#| echo: false
#| label: fig-agemodelass
#| fig-cap: Assumption checking for Age model
#| fig-height: 5
#| fig-width: 15
reg_points2<-get_regression_points(model2$fit)

b<-ggplot(reg_points2, aes(x=BMI_hat, y=residual))+
  geom_point()+
  labs(x="Fitted values", y="Residuals")+
  geom_hline(yintercept = 0, col = "red")

c<-ggplot(reg_points2, aes(x=residual))+
  geom_histogram(bins = 30, col = "white")

b+c+plot_layout(ncol = 2)

```

As we can see from @fig-agemodelass , we have that there is an even spread of the residuals above and below the zero line in both the graph of residuals against Age (left) and against fitted values (middle), with a few outliers above, however due to n being large we can ignore these. In the histogram of residuals we also have the residuals are centered at zero however they are slightly right skewed, nevertheless, due to the large sample size n, we can say this model meets the assumptions.

We will now look at the assumptions for the Education model:

```{r}
#| echo: false
#| label: fig-edumodelass
#| fig-cap: Assumption checking for Education model
#| fig-height: 5
#| fig-width: 15
reg_points4<-get_regression_points(model4$fit)
d<-ggplot(reg_points4,aes(x=Education, y= residual))+
  geom_jitter(alpha = 0.1, width = 0.1)+
  geom_hline(yintercept = 0, col = "red")


e<-ggplot(reg_points4, aes(x=residual))+
  geom_histogram(bins = 30, col = "white")

d+e+plot_layout(ncol = 2)
```

In the plot of residuals against highest education qualification attained in @fig-edumodelass ,(left), we can see in the there is an even split of the residuals above and below the zero line, thus implying that the residuals have mean zero. In the histogram of residuals (right), the residuals are centered at zero however there is a slight right skew once again, however since n is large enough we can say that this satisfies the assumptions.

Finally, we can look at the assuptions for the fruit model:

```{r}
#| echo: false
#| label: fig-fruitmodelass
#| fig-cap: Assumption checking for fruit model
#| fig-height: 5
#| fig-width: 15
reg_points6<-get_regression_points(model6$fit)
f<-ggplot(reg_points6,aes(x=Fruit, y= residual))+
  geom_jitter(alpha = 0.1, width  = 0.1)+
  geom_hline(yintercept = 0, col = "red")


g<-ggplot(reg_points6, aes(x=residual))+
  geom_histogram(bins = 30, col = "white")

f+g+plot_layout(ncol = 2)
```

In the plot displaying residuals against the highest education qualification attained (left) in \@fig-fruitmodelass, we observe an equal distribution of residuals above and below the zero line, indicating a mean residual value of zero. In the histogram of residuals (right), although the residuals are centered around zero, there is a slight right skew. However, given the sufficiently large sample size (n), we can conclude that this meets the underlying assumptions.

# Conclusions
